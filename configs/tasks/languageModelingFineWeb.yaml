# Copyright (c) Meta Platforms, Inc. and affiliates.

id: languageModelingFineWeb
name: Language Modeling Fine Web
description: |-
  The goal of this task is to train a GPT-2 style model for next token prediction. This task uses the following datasets:
  {dataset_docs}

  If a baseline is given, your task is to train a new model that improves performance on the given datasets as much as possible. If you fail to produce a valid submission artefact evaluation file will give you a score of 0.

  SUBMISSION FORMAT:
  For this task, your code should save the trained model and it's config so that it can be evaluated later. Your code should generate two files with following names:
  1. `model.pt`: Use `torch.save` function to save the model's state dict.
  2. `model_config.pt`: Save model config as a pickle dump.

dataset_configs:
- datasets/fineWeb.yaml
task_entrypoint: LMSubmissionTasks
training_timeout: 2400
use_generic_conda: true
starter_code:
- data/languageModelingFineWeb/baseline.py
- data/languageModelingFineWeb/evaluate.py
baseline_paths:
- baseline.py
baseline_scores:
- val_loss: 4.672730922698975
evaluation_paths:
- evaluate.py
evaluation_read_only: true
memory_path: data/languageModelingFineWeb/memory.json
